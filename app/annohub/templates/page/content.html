<h1 id="introduction">Introduction</h1>
<p>I made the observation, that forensic linguists are frequently annotating data. Unfortunately not all the processing steps are done in the same way, so the results might not be reproducible. Besides that, a lot of implicit decision may lead to incomparable results and that should change.</p>
<h1 id="annohub">Annohub</h1>
<ul>
<li>our goal is to make the implicit steps explicit and you should do as less work as possible</li>
<li>we do the heavy lifting: preprocessing, segmentation and initial annotation</li>
<li>we would like to esablish a platform where people can share their process and data</li>
</ul>
<h2 id="design-considerations">Design Considerations</h2>
<ul>
<li>be flexible and support multiple languages and tagsets</li>
<li>be transparent about the annotation process</li>
<li>users should always get their data back</li>
</ul>
<h2 id="features">Features</h2>
<ul>
<li>tokenization gui</li>
<li>annotation gui</li>
<li>manage projects and annotators</li>
<li>export your data</li>
<li>stats for your project</li>
</ul>
<h2 id="we-want-to-encourage-you">We Want To Encourage You</h2>
<ul>
<li>to learn NLTK and Python</li>
<li>to host annohub yourself (batteries are included!)</li>
<li>to contribute to annohub’s development</li>
<li>to adapt annohub to your personal requirements</li>
<li>to improve NLTK by providing tagset-mappings or by training taggers</li>
</ul>
<h1 id="how-to">How-to</h1>
<p>Every project has different steps that you have to complete. For some of them you can find additional details in the sections below. The following list is an overview of the lifecycle of each project:</p>
<ul>
<li><strong>creation</strong>: the creator uploads the text. He chooses additional data that helps to process the text: the genre, the language and a tagset.</li>
<li><strong>tokenization</strong>: the creator of a project has to decide what the sentences and the token are. Also words sentences and punctuation can be separated or completely deleted from the text. It is also possible to join punctuation, tokens or sentences so mistakes of the automated segmentation can be corrected.</li>
<li><strong>manage annotators</strong>: the creator can invite annotators now. Only annotators can annotate the project and have to be invited, even if the creator and the annotator are the same person. The creator has to finally choose a tagset here but currently only one tagset is available.</li>
<li><strong>annotation</strong>: this is the main step for annotators you will do most of the work.
<ul>
<li>the menu on the top enables annotators to display information related to the tag by hovering over it. A different tag is chosen by clicking on it in the menu.</li>
<li>it is also possible to select a tag by typing it into the search bar. This also displays the description of the currently selected tag which can be chosen by pressing enter.</li>
</ul></li>
<li><strong>publish</strong> and display <strong>statistics</strong>: After the annotation you can select a license and publish your project. After this step you and your annotators can view basic statistics about your project. Published projects are currently not listed. TODO: implement stats.</li>
</ul>
<p>It is always possible to <strong>export</strong> the project. This is also useful for annotators that might be removed from the project and permits them to save their data. TODO: implement export.Projects can also be deleted or transfered. TODO: implement transfer.</p>
<h1 id="preprocessing">Preprocessing</h1>
<p>What are typical preprocessing steps?</p>
<ul>
<li>Somehow the data that should be processed has to be read, so we have to handle different kinds of encodings.</li>
<li>After we successfully read the data, we might want to “clean” it. What you do while preprocessing depends on your needs, but how you do it should be consistent and in the best case the tools you used should be accessible to everyone.</li>
<li>However there are preprocessing steps that might be problematic, because they are irreversibly changing the data. Stripping punctuation and the practice of writing uppercase characters as lowercase are examples for this.</li>
</ul>
<h2 id="punctuation-and-lowercase">Punctuation And Lowercase</h2>
<p><span class="citation">Manning and Schütze (1999, p. 124)</span> write the following about stripping inter-sentence-punctuation:</p>
<blockquote>
<p>While normally people want to keep sentence boundaries (see section 4.2.4 below), often sentence-internal punctuation has just been stripped out. This is probably unwise. Recent work has emphasized the information contained in all punctuation. No matter how imperfect a representation, punctuation marks like commas and dashes give some clues about the macro structure of the text and what is likely to modify what.</p>
</blockquote>
<p>This is exactly the point why there is a warning sign next to this preprocessing option: Do you want to lose clues about the macro structure? Have a look at the first paragraphs of <a href="http://www.gutenberg.org/cache/epub/11/pg11.txt">Alice in Wonderland</a>, which includes a lot of embedded speech and thoughts, and also some words are uppercase which gives them a different emphasis:</p>
<pre><code>CHAPTER I. Down the Rabbit-Hole

Alice was beginning to get very tired of sitting by her sister on the
bank, and of having nothing to do: once or twice she had peeped into the
book her sister was reading, but it had no pictures or conversations in
it, &#39;and what is the use of a book,&#39; thought Alice &#39;without pictures or
conversations?&#39;

So she was considering in her own mind (as well as she could, for the
hot day made her feel very sleepy and stupid), whether the pleasure
of making a daisy-chain would be worth the trouble of getting up and
picking the daisies, when suddenly a White Rabbit with pink eyes ran
close by her.

There was nothing so VERY remarkable in that; nor did Alice think it so
VERY much out of the way to hear the Rabbit say to itself, &#39;Oh dear!
Oh dear! I shall be late!&#39; (when she thought it over afterwards, it
occurred to her that she ought to have wondered at this, but at the time
it all seemed quite natural); but when the Rabbit actually TOOK A WATCH
OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,
Alice started to her feet, for it flashed across her mind that she had
never before seen a rabbit with either a waistcoat-pocket, or a watch
to take out of it, and burning with curiosity, she ran across the field
after it, and fortunately was just in time to see it pop down a large
rabbit-hole under the hedge.</code></pre>
<p>Clearly we would not be able to find good sentence boundaries for the last sentence if we stripped the markers for embedded speech.</p>
<h1 id="tokenization">Tokenization</h1>
<ul>
<li>all the available tokenizers are based on punctuation</li>
<li>because of that, the preprocessing option <code>strip punctuation</code> has a warning sign: implemented carelessly it would mess up the segmentation of sentences.</li>
<li><code>strip punctuation</code> excludes punctuation that is used to end a sentence, defined by the <code>delimiter_list = ['?', '!', '.', '…']</code> in <code>preprocessor.py</code></li>
<li>also it won’t strip hyphens that mark a compound like <em>waistcoat-pocket</em></li>
<li>tokenizers for the following languages are available:
<ul>
<li>czech</li>
<li>danish</li>
<li>dutch</li>
<li>english</li>
<li>estonian</li>
<li>finnish</li>
<li>french</li>
<li>german</li>
<li>greek</li>
<li>italian</li>
<li>norwegian</li>
<li>polish</li>
<li>portuguese</li>
<li>slovene</li>
<li>spanish</li>
<li>swedish</li>
<li>turkish</li>
</ul></li>
</ul>
<h2 id="tokenization-background">Tokenization Background</h2>
<p>We have our cleaned data now, time to decide what a sentence is (sentence segmentation) and what a word is (tokenization). This is sounds very easy at first: Words are separated by whitespaces. But this is not always the case and there are cases that are hard to decide but need to be handled consistently. Again, from <span class="citation">Manning and Schütze (1999, p. 124)</span>:</p>
<blockquote>
<p>What is a humble computational linguist meant to do? Kucera and Francis (1967) suggested the practical notion of a graphic word which they define as a string of contiguous alphanumeric characters with space on either side; may include hyphens and apostrophes, but no other punctuation marks. But, unfortunately, life is not that simple, even if one is just looking for a practical, workable definition. Kucera and Francis seem in practice to use intuition, since they regard as words numbers and monetary amounts like $22.50 which do not strictly seem to obey the definition above.</p>
</blockquote>
<p>Also, not all languages separate words by whitespaces and “the question of what counts as a word is a vexed one in linguistics, and often linguists end up suggesting that there are words at various levels, such as phonological words versus syntactic words, which need not all be the same.” <span class="citation">(Manning and Schütze, 1999, p.125)</span></p>
<h2 id="annotation-and-agreement">Annotation And Agreement</h2>
<p>After these steps we can finally start our annotation. Obviously we want to minimize the work we have to do, that’s why we start with an automated approach that chooses the correct annotation most of the time. But most of the time means: There will be errors and we would like to know about the algorithm, the training the tagger got, better paramaters we could choose and the error-rate. Finally we have to review the output of the tagger and correct it’s mistakes. That is one of the main jobs of annohub: Provide a nice interface that enables the annotator to choose the matching tag most efficiently.</p>
<p>Human annotators make errors too and typicly we are looking at the <a href="https://corpuslinguisticmethods.wordpress.com/2014/01/15/what-is-inter-annotator-agreement/">inter-annotator agreement</a> to decide if we can actually use an annotation.</p>
<h2 id="languages">Languages</h2>
<p>Unfortunately the available taggers are only trained with english language data from f.e. :</p>
<ul>
<li>the <a href="https://www.cis.upenn.edu/~treebank/">Penn Treebank</a>,</li>
<li>the <a href="https://catalog.ldc.upenn.edu/LDC2000T43">Wall Street Journal</a> (WSJ)</li>
<li>or the <a href="http://www.hit.uib.no/icame/brown/bcm.html">Brown Corpus</a></li>
</ul>
<p>Information about the data that was used to train the tagger is really hard to find, because only the pickled tagger is available.</p>
<p>That means genres not included there might not work as good and the situation for languages other than english is a little bit … embarassing?</p>
<h2 id="tagsets">Tagsets</h2>
<p>Documentation for three tagsets is included in NLTK:</p>
<ul>
<li>Penn Treebank (upenn)</li>
<li>Brown-Corpus (brown)</li>
<li>Claws-5 (claws5)</li>
</ul>
<p>The default tagger was trained with the <code>upenn</code> tagset on the <code>WSJ</code> corpus (<a href="https://spacy.io/blog/part-of-speech-POS-tagger-in-python%3E">PerceptronTagger</a>). So in order to use a different tagset than the <code>upenn</code> tagset, two solutions are possible: mappings and training.</p>
<h2 id="mappings">Mappings</h2>
<p>A mapping maps the tags of one tagset to the tags of another tagset (an at least surjective function). NLTK can load and <a href="http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.mapping">map</a> between two tagsets. Mappings have the following form and currently exist only <strong>to</strong> the <a href="http://arxiv.org/abs/1104.2086">Universal Tagset</a>.</p>
<pre><code>ABL PRT
ABN PRT
ABN-HL  PRT
ABN-NC  PRT
ABN-TL  PRT
ABX DET
AP  ADJ
AP$ PRT
AP+AP-NC    ADJ
AP-HL   ADJ
AP-NC   ADJ
AP-TL   ADJ
AT  DET</code></pre>
<p>As you can see from the example, the Universal Tagset is by intention very general and maps multiple tags from the Brown Corpus to the same tag That means detailed linguistic information is lost and as a consequence, you won’t be able to map the universal tagset back (mapping to the Universal Tagset is not a bijection). If you don’t want that to happen, you have to write your own mapping from the <code>upenn</code> tagset to whatever tagset you want to use.</p>
<p>Fortunately other people (Nathan Schneider) already put some work into that:</p>
<ul>
<li><a href="https://gist.github.com/nschneid/4231292" class="uri">https://gist.github.com/nschneid/4231292</a></li>
<li><a href="https://gist.github.com/nschneid/6476715" class="uri">https://gist.github.com/nschneid/6476715</a></li>
</ul>
<p>but I have not tested these scripts or verfied that they do a good job. It also would be nice to include the mappings into NLTK, I think they would be happy about reliable mappings.</p>
<p>As an additional idea: Why shouldn’t we create a very detailed tagset that can represent all the other tagsets? We could use it as a mapping tagset so we would be able to map from every other tagset to the mapping tagset and from the mapping tagset to every other tagset. That would greatly simplify all mapping-worries, right?</p>
<h2 id="training">Training</h2>
<p>If you really want to have better support for other languages you should start to train the <a href="http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.perceptron">Perceptron-Tagger</a>, see <code>train()</code> and make your results available to NLTK.</p>
<p>There also is <a href="http://nltk-trainer.readthedocs.org/en/latest/train_tagger.html">nltk-trainer</a>, which might be helpful, or can give you ideas (see <a href="https://github.com/japerk/nltk-trainer/blob/master/train_tagger.py%3E">train_tagger.py</a>) on how to train an HMM-Tagger.</p>
<p>Training taggers needs a lot of data, the more the better (the author of the Perceptron-Tagger talks about 5000 examples, but that is only an example by itself). If we would go to conventional way, we would use like 95% of the <code>WSJ</code> Corpus (~47M words) for training, and we would need 44.650.000 words with the correct part of speech label in our language. Read more about training in the NLTK book, <a href="http://www.nltk.org/book/ch06.html">about classifying text</a>.</p>
<p>If you have ideas about training taggers or are working on a mapping our would like to write the Mapping Tagset outlined above, write me.</p>
<h1 id="developer-documentation">Developer Documentation</h1>
<h2 id="extensibility">Extensibility</h2>
<ul>
<li>the app is separated into blueprints</li>
<li>the library is separated into componants that are testable</li>
<li>the nlp parts are separated in dedicated modules and can also be changed easily</li>
</ul>
<h2 id="open-source">Open Source</h2>
<ul>
<li>join us on GitHub (looking for contributors!)</li>
<li>please submit a ticket if you find an issue</li>
<li>ideas! fixes! Pull requests are welcome!</li>
</ul>
<h2 id="tests">Tests</h2>
<ul>
<li>some tasks are covered by seleniumtests</li>
<li>library is covered by unittests</li>
</ul>
<h2 id="db">DB</h2>
<ul>
<li>saving individual sentences increases the performance of MongoDB</li>
<li>other approaches with embedded documents or growing list fields were not performant enough</li>
<li>data can be deduplicated more easily</li>
<li>MongoDB can scale TODO: limit</li>
</ul>
<h1 id="wishlist">Wishlist</h1>
<ul>
<li>let the user add arbitrary annotations</li>
<li>more languages, more tagsets, more kinds of annotation</li>
<li>implement a plugin system that administrators can use</li>
<li>implement imports and a REST API</li>
<li>translate the gui to different languages</li>
</ul>
<div id="references" class="references">
<h1 id="references" class="unnumbered">References</h1>
<div id="ref-manning_foundations_1999">
<p>Christopher D. Manning and Hinrich Schütze. 1999. <em>Foundations of statistical natural language processing</em>. MIT Press, Cambridge, MA, USA, editions.</p>
</div>
</div>
